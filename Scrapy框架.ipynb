{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy\n",
    "# 爬虫框架\n",
    "- 框架\n",
    "- 爬虫框架\n",
    "    - scrapy \n",
    "    - pyspider\n",
    "    - crawley\n",
    "- scrapy框架介绍\n",
    "    - https://doc.scrapy.org/en/latest/\n",
    "    - http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html\n",
    "    \n",
    "- 安装\n",
    "    - 利用pip  \n",
    " \n",
    "- scrapy概述\n",
    "    - 包含各个部件\n",
    "        - ScrapyEngine： 神经中枢，大脑，核心、\n",
    "        - Scheduler调度器：引擎发来的request请求，调度器需要处理，然后交换引擎\n",
    "        - Downloader下载器：把引擎发来的requests发出请求，得到response\n",
    "        - Spider爬虫： 负责把下载器得到的网页/结果进行分解，分解成数据+链接\n",
    "        - ItemPipeline管道： 详细处理Item\n",
    "        - DownloaderMiddleware下载中间件： 自定义下载的功能扩展组件\n",
    "        - SpiderMiddleware爬虫中间件：对spider进行功能扩展\n",
    "        \n",
    "- 爬虫项目大概流程\n",
    "    - 新建项目：scrapy startproject xxx\n",
    "    - 明确需要目标/产出:  编写item.py\n",
    "    - 制作爬虫 ： 地址 spider/xxspider.py\n",
    "    -  存储内容： pipelines.py,   \n",
    "    \n",
    "- ItemPipeline\n",
    "    - 对应的是pipelines文件\n",
    "    - 爬虫提取出数据存入item后，item中保存的数据需要进一步处理，比如清洗，去重，存储等\n",
    "    - process_item:\n",
    "        - spider提取出来的item作为参数传入，同时传入的还有spider\n",
    "        - 此方法必须实现\n",
    "        - 必须返回一个Item对象，被丢弃的item不会被之后的pipeline处理\n",
    "    - __init__:构造函数\n",
    "        - 进行一些必要的参数初始化     \n",
    "    - open_spider(spider):\n",
    "        - spider对象被开启的时候调用\n",
    "    - close_spider(spider):\n",
    "        - 当spider对象被关闭的时候调用 \n",
    "- Spider\n",
    "    - 对应的是文件夹spiders下的文件\n",
    "    - __init__: 初始化爬虫名称，start_urls列表\n",
    "    - start_requests:生成Requests对象交给Scrapy下载并返回response\n",
    "    - parse： 根据返回的response解析出相应的item，item自动进入pipeline； 如果需要，解析出url，url自动交给\n",
    "    requests模块，一直循环下去\n",
    "    - start_request: 此方法仅能被调用一次，读取start_urls内容并启动循环过程\n",
    "    - name:设置爬虫名称\n",
    "    - start_urls:  设置开始第一批爬取的url\n",
    "    - allow_domains:spider允许爬去的域名列表\n",
    "    - start_request(self)： 只被调用一次\n",
    "    - parse\n",
    "    - log:日志记录\n",
    "- 中间件(DownloaderMiddlewares)\n",
    "    - 中间件是处于引擎和下载器中间的一层组件\n",
    "    - 可以有很多个，被按顺序加载执行\n",
    "    - 作用是对发出的请求和返回的结果进行预处理\n",
    "    - 在middlewares文件中\n",
    "    - 需要在settings中设置以便生效\n",
    "    - 一般一个中间件完成一项功能\n",
    "    - 必须实现以下一个或者多个方法\n",
    "        - process_request(self, request, spider)\n",
    "            - 在request通过的时候被调用\n",
    "            - 必须返回None或Response或Request或raise IgnoreRequest\n",
    "            - None: scrapy将继续处理该request\n",
    "            - Request： scrapy会停止调用process_request并冲洗调度返回的reqeust\n",
    "            - Response： scrapy不会调用其他的process_request或者process_exception，直接讲该response作为结果返回\n",
    "            同时会调用process_response函数\n",
    "        - process_response(self, request, response,  spider)\n",
    "            - 跟process_request大同小异\n",
    "            - 每次返回结果的时候会自动调用\n",
    "            - 可以有多个，按顺序调用\n",
    "        - 案例代码\n",
    "        \n",
    "                import random\n",
    "                import base64\n",
    "                \n",
    "                # 从settings设置文件中导入值\n",
    "                from settings import USER_AGENTS\n",
    "                from settings import PROXIES\n",
    "                \n",
    "                #  随机的 User-Agent\n",
    "                class RandomUserAgent(object):\n",
    "                    def process_request(self, request, spider):\n",
    "                        useragent = random.choice(USER_AGENTS)\n",
    "                        request.headers.setdefault(\"User-Agent\", useragent)\n",
    "                        \n",
    "                class RandomProxy(object):\n",
    "                    def process_request(self, request, spider):\n",
    "                        proxy = random.choice(PROXIES)\n",
    "                        if proxy['user_passwd'] is None:\n",
    "                            #  没有代理账户验证的代理使用方式\n",
    "                            request.meta['proxy'] = \"http://\" + proxy['ip_port']\n",
    "                        else:\n",
    "                            #  对账户密码进行 base64 编码转换\n",
    "                            base64_userpasswd = base64.b64encode(proxy['user_passwd'])\n",
    "                            #  对应到代理服务器的信令格式里\n",
    "                            request.headers['Proxy-Authorization'] = 'Basic ' + base64_userpasswd\n",
    "                            request.meta['proxy'] = \"http://\" + proxy['ip_port']\n",
    "        - 设置settings的相关代码\n",
    "        \n",
    "        \n",
    "                USER_AGENTS = [\n",
    "                            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR\n",
    "                            3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "                            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0;\n",
    "                            SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET\n",
    "                            CLR 1.1.4322)\",\n",
    "                            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR\n",
    "                            2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "                            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko,\n",
    "                            Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "                            \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3)\n",
    "                            Arora/0.6\",\n",
    "                            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-\n",
    "                            Ninja/2.1.1\",\n",
    "                            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0\n",
    "                            Kapiko/3.0\",\n",
    "                            \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\"\n",
    "                            ]           \n",
    "   \n",
    "                PROXIES = [\n",
    "                        {'ip_port': '111.8.60.9:8123', 'user_passwd': 'user1:pass1'},\n",
    "                        {'ip_port': '101.71.27.120:80', 'user_passwd': 'user2:pass2'},\n",
    "                        {'ip_port': '122.96.59.104:80', 'user_passwd': 'user3:pass3'},\n",
    "                        {'ip_port': '122.224.249.122:8088', 'user_passwd': 'user4:pass4'},\n",
    "                        ]\n",
    "- 去重\n",
    "    - 为了放置爬虫陷入死循环，需要去重\n",
    "    - 即在spider中的parse函数中，返回Request的时候加上dont_filter=False参数\n",
    "    \n",
    "            myspeder(scrapy.Spider):\n",
    "                def parse(.....):\n",
    "                \n",
    "                    ......\n",
    "                    \n",
    "                    yield  scrapy.Request(url=url, callback=self.parse, dont_filter=False)                \n",
    "       \n",
    "- 如何在scrapy使用selenium\n",
    "    - 可以放入中间件中的process_request函数中\n",
    "    - 在函数中调用selenium，完成爬取后返回Response\n",
    "    \n",
    "        \n",
    "            calss MyMiddleWare(object):\n",
    "                def process_request(.....):\n",
    "                    \n",
    "                    driver = webdriver.Chrome()\n",
    "                    html = driver.page_source\n",
    "                    driver.quit()\n",
    "                    \n",
    "                    return HtmlResponse(url=request.url, encoding='utf-8', body=html, request=request)\n",
    "                    \n",
    "\n",
    "            \n",
    "- 案例e16-qq招聘\n",
    "    - 创建项目\n",
    "    - 编写item\n",
    "    - 编写spider\n",
    "    - 编写pipeline\n",
    "    - 设置pipeline\n",
    "     \n",
    "- 案例 e14-scrapy-baidu\n",
    "    - 利用最简单的爬虫\n",
    "    - 爬去百度页面\n",
    "    - 关闭配置机器人协议\n",
    "    - scrapy startproject baidu\n",
    "    - scrapy crawl baidu\n",
    "    \n",
    "- 案例e15-meiju\n",
    "    - 创建新项目\n",
    "    - 分析网页，定义item\n",
    "    - 编写pipeline， 确定如何处理item\n",
    "    - 编写spider， 确定如何提取item\n",
    "    - 可以通过增加一个单独命令文件的方式在pycharm中启动爬虫\n",
    "    \n",
    "- 案例e17-校花网\n",
    "    - 创建项目\n",
    "    - 编写item\n",
    "    - spider\n",
    "    - pipeline\n",
    "    - 设置pipeline\n",
    "    - 中间件， 会使用selenium"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
